{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products = pd.read_pickle('products')\n",
    "priors = pd.read_pickle('priors')\n",
    "users = pd.read_pickle('users')\n",
    "userXproduct = pd.read_pickle('userXproduct')\n",
    "df_temp = pd.read_pickle('df_temp')\n",
    "df_train = pd.read_pickle('df_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_fun(labels, preds):\n",
    "    labels = labels.split(' ')\n",
    "    preds = preds.split(' ')\n",
    "    rr = (np.intersect1d(labels, preds))\n",
    "    precision = np.float(len(rr)) / len(preds)\n",
    "    recall = np.float(len(rr)) / len(labels)\n",
    "    try:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "        return (precision, recall, 0.0)\n",
    "    return (precision, recall, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 交叉验证实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#一些辅助函数 为了交叉验证 需要把同一订单号的 物品 合为一个\n",
    "def get_liststr(df_test):\n",
    "    n=1\n",
    "    for row in df_test:\n",
    "        if n==1:\n",
    "            temp=str(row)\n",
    "            n=0\n",
    "             \n",
    "        else:\n",
    "                temp += ' ' + str(row)\n",
    "    return  temp\n",
    "\n",
    "def get_liststr1(df_test):\n",
    "    n=1\n",
    "    for row in df_test.split(' '):\n",
    "        if n==1:\n",
    "            temp=row\n",
    "            n=0\n",
    "             \n",
    "        else:\n",
    "                temp += ' ' + row\n",
    "    return  temp\n",
    "#把预测结果 通过阈值 挑选出来 放入一个字符串中 ‘product1 2 3 4···’\n",
    "def get_pred_results(df_test,thrshold=0.22):\n",
    "    TRESHOLD = thrshold  # guess, should be tuned with crossval on a subset of train data\n",
    "\n",
    "    d = dict()\n",
    "    for row in df_test.itertuples():\n",
    "        if row.pred > TRESHOLD:\n",
    "            try:\n",
    "                d[row.order_id] += ' ' + str(row.product_id)\n",
    "            except:\n",
    "                d[row.order_id] = str(row.product_id)\n",
    "\n",
    "    for order in df_test.order_id:\n",
    "        if order not in d:\n",
    "            d[order] = 'None'\n",
    "\n",
    "    sub = pd.DataFrame.from_dict(d, orient='index')\n",
    "    sub.reset_index(inplace=True)\n",
    "    sub.columns = ['order_id', 'products']\n",
    "    return sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = np.array(df_train['labels'],dtype=pd.Series)\n",
    "df_train.drop(['labels'],axis=1,inplace=True)\n",
    "\n",
    "f_to_use = ['user_total_orders', 'user_total_items', 'total_distinct_items',\n",
    "       'user_average_days_between_orders', 'user_average_basket',\n",
    "       'order_hour_of_day', 'days_since_prior_order', #'days_since_ratio',\n",
    "       'aisle_id', 'department_id', 'product_orders', 'product_reorders',\n",
    "       'product_reorder_rate', 'UP_orders', 'UP_orders_ratio',\n",
    "       'UP_average_pos_in_cart', 'UP_reorder_rate', 'UP_orders_since_last',\n",
    "       'UP_delta_hour_vs_last'] # 'dow', 'UP_same_dow_as_last_order'\n",
    "\n",
    "\n",
    "\n",
    "#lgb.plot_importance(bst, figsize=(9,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fscore(df,bst,alpha):\n",
    "    df['pred'] = bst.predict(df[f_to_use])\n",
    "    train_pred=get_pred_results(df,thrshold=alpha)\n",
    "    #合表\n",
    "    train_pred1=pd.merge(train_pred,df_temp,on=['order_id'])\n",
    "    #求F1结果表\n",
    "    res = list()\n",
    "    for entry in train_pred1.itertuples():\n",
    "        res.append(eval_fun(entry[2], entry[3]))\n",
    "    res = pd.DataFrame(np.array(res), columns=['precision', 'recall', 'f1'])\n",
    "    return res['f1'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fscore_xgb(df,bst,alpha):\n",
    "    d_d=xgb.DMatrix(df[f_to_use])\n",
    "    df['pred'] = bst.predict(d_d)\n",
    "    train_pred=get_pred_results(df,thrshold=alpha)\n",
    "    #合表\n",
    "    train_pred1=pd.merge(train_pred,df_temp,on=['order_id'])\n",
    "    #求F1结果表\n",
    "    res = list()\n",
    "    for entry in train_pred1.itertuples():\n",
    "        res.append(eval_fun(entry[2], entry[3]))\n",
    "    res = pd.DataFrame(np.array(res), columns=['precision', 'recall', 'f1'])\n",
    "    return res['f1'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fscore_nn(data, pred, model, alpha):\n",
    "    data['pred'] = pred\n",
    "    data_pred = get_pred_results(data, thrshold=alpha)\n",
    "    # 合表\n",
    "    data_pred1 = pd.merge(data_pred, df_temp, on=['order_id'])\n",
    "    # 求F1结果表\n",
    "    res = list()\n",
    "    for entry in data_pred1.itertuples():\n",
    "        res.append(eval_fun(entry[2], entry[3]))\n",
    "    res = pd.DataFrame(np.array(res), columns=['precision', 'recall', 'f1'])\n",
    "    return res[\"precision\"].mean(), res['recall'].mean(), res['f1'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#先使用最简单的k-Fold\n",
    "from sklearn.model_selection import KFold\n",
    "from lightgbm import LGBMClassifier\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': {'binary_logloss'},\n",
    "    'num_leaves': 96,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.95,\n",
    "    'bagging_freq': 5\n",
    "}\n",
    "ROUNDS = 98\n",
    "kf=KFold(n_splits=3)    # 定义分成几个组\n",
    "list_f1=[]\n",
    "list_f2=[]\n",
    "num=1\n",
    "#clf=LGBMClassifier(objective='binary', boosting_type='gbdt')\n",
    "#决定采用手动cv  因为需要了利用合表才能得到F1 传统方法不可以 数组合表 太可怕···\n",
    "#把数组变成切边的形式  即可\n",
    "import timeit\n",
    "start=timeit.default_timer()\n",
    "\n",
    "for train_index,test_index in kf.split(df_train, labels):\n",
    "    \n",
    "    #train_max=train_index.max()\n",
    "    #train_min=train_index.min()\n",
    "    test_max=test_index.max()+1\n",
    "    test_min=test_index.min()\n",
    "    X_test=df_train[test_min:test_max]\n",
    "    X_train=df_train.drop(test_index)\n",
    "    #X_train,X_test=data_train[train_index],data_train[test_index]\n",
    "    y_train,y_test=labels[train_index],labels[test_index]   \n",
    "    d_train = lgb.Dataset(X_train[f_to_use],\n",
    "                      label=y_train,\n",
    "                      categorical_feature=['aisle_id', 'department_id'])  # , 'order_hour_of_day', 'dow'\n",
    "    \n",
    "    \n",
    "    bst = lgb.train(params, d_train, ROUNDS, early_stopping_rounds=50)\n",
    "    a=fscore(X_train,bst,0.22)\n",
    "    b=fscore(X_test,bst,0.22)\n",
    "    list_f1.append(a)\n",
    "    list_f2.append(b)\n",
    "    print('* {}: train:{}, test:{}'.format(num,a,b))\n",
    "    num+=1\n",
    "    \n",
    "print('ALL:train:{} test:{}'.format(np.mean(list_f1),np.mean(list_f2)))\n",
    "end = timeit.default_timer()\n",
    "print('cost time:'+str(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#先使用最简单的k-Fold\n",
    "from sklearn.model_selection import KFold\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import xgboost as xgb\n",
    "params_xgb ={\n",
    "  \"objective\"  : \"reg:logistic\",\n",
    "  \"eval_metric\"   : \"logloss\",\n",
    "  \"eta\"    :0.1,\n",
    "  \"max_depth\"   : 7,\n",
    "  \"min_child_weight\"  : 3,\n",
    "  \"gamma\"             : 0.70,\n",
    "  \"subsample\"          : 0.78,\n",
    "  \"colsample_bytree\"    : 0.95,\n",
    "  \"alpha\"             : 2e-05,\n",
    "  \"lambda\"            : 10\n",
    "        }\n",
    "kf=KFold(n_splits=3)    # 定义分成几个组\n",
    "list_f1=[]\n",
    "list_f2=[]\n",
    "num=1\n",
    "\n",
    "#clf=LGBMClassifier(objective='binary', boosting_type='gbdt')\n",
    "#决定采用手动cv  因为需要了利用合表才能得到F1 传统方法不可以 数组合表 太可怕···\n",
    "#把数组变成切边的形式  即可\n",
    "import timeit\n",
    "start=timeit.default_timer()\n",
    "print('start cv :-) long time```')\n",
    "count = 0\n",
    "\n",
    "for train_index,test_index in kf.split(df_train, labels):    \n",
    "    test_max=test_index.max()+1\n",
    "    test_min=test_index.min()\n",
    "    X_test=df_train[test_min:test_max]\n",
    "    X_train=df_train.drop(test_index)\n",
    "    y_train,y_test=labels[train_index],labels[test_index]   \n",
    "    d_train = xgb.DMatrix(X_train[f_to_use],\n",
    "                      label=y_train)\n",
    "    \n",
    "    ROUNDS = 98\n",
    "    bst = xgb.train(params_xgb, d_train, ROUNDS)\n",
    "    a=fscore_xgb(X_train,bst,0.22)\n",
    "    b=fscore_xgb(X_test,bst,0.22)\n",
    "    list_f1.append(a)\n",
    "    list_f2.append(b)\n",
    "    print('* {}: train:{}, test:{}'.format(num,a,b))\n",
    "    num+=1\n",
    "    count += 1\n",
    "    if count >= 1:\n",
    "        break\n",
    "                                       \n",
    "    \n",
    "print('ALL:train:{} test:{}'.format(np.mean(list_f1),np.mean(list_f2)))\n",
    "end = timeit.default_timer()\n",
    "print('cost time:'+str(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is enabled with initial size: 20.0% of memory, cuDNN 5005)\n",
      "/usr/local/lib/python3.4/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn import cross_validation\n",
    "from sklearn import preprocessing\n",
    "\n",
    "train, test, train_labels, test_labels = cross_validation.train_test_split(df_train, labels, test_size=0.3, random_state=0)\n",
    "\n",
    "scalerX = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "train_x = scalerX.fit_transform(train[f_to_use])\n",
    "test_x = scalerX.transform(test[f_to_use])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=18, activation='relu'))\n",
    "    model.add(Dense(256, input_dim=128, activation='relu'))\n",
    "    model.add(Dense(64, input_dim=128, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    from keras import backend as K\n",
    "    def weight_crossentropy(y_true, y_pred):\n",
    "        return K.mean(-(y_true * K.log(y_pred)*1.8 + (1.0 - y_true) * K.log(1.0 - y_pred)*0.2), axis=-1)\n",
    "\n",
    "    model.compile(loss=weight_crossentropy, optimizer=\"rmsprop\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5339035 samples, validate on 593227 samples\n",
      "Epoch 1/15\n",
      "5339035/5339035 [==============================] - 19s - loss: 0.1961 - acc: 0.7265 - val_loss: 0.1894 - val_acc: 0.6979\n",
      "Epoch 2/15\n",
      "5339035/5339035 [==============================] - 18s - loss: 0.1897 - acc: 0.7337 - val_loss: 0.1886 - val_acc: 0.7922\n",
      "Epoch 3/15\n",
      "5339035/5339035 [==============================] - 19s - loss: 0.1880 - acc: 0.7357 - val_loss: 0.1867 - val_acc: 0.7942\n",
      "Epoch 4/15\n",
      "5339035/5339035 [==============================] - 19s - loss: 0.1871 - acc: 0.7393 - val_loss: 0.1845 - val_acc: 0.7570\n",
      "Epoch 5/15\n",
      "5339035/5339035 [==============================] - 19s - loss: 0.1863 - acc: 0.7412 - val_loss: 0.1835 - val_acc: 0.7363\n",
      "Epoch 6/15\n",
      "5339035/5339035 [==============================] - 19s - loss: 0.1857 - acc: 0.7426 - val_loss: 0.1845 - val_acc: 0.7851\n",
      "Epoch 7/15\n",
      "5339035/5339035 [==============================] - 19s - loss: 0.1853 - acc: 0.7420 - val_loss: 0.1846 - val_acc: 0.7994\n",
      "Epoch 8/15\n",
      "5339035/5339035 [==============================] - 19s - loss: 0.1849 - acc: 0.7431 - val_loss: 0.1826 - val_acc: 0.7746\n",
      "Epoch 9/15\n",
      "5339035/5339035 [==============================] - 19s - loss: 0.1846 - acc: 0.7443 - val_loss: 0.1851 - val_acc: 0.6806\n",
      "Epoch 10/15\n",
      "5339035/5339035 [==============================] - 19s - loss: 0.1843 - acc: 0.7441 - val_loss: 0.1893 - val_acc: 0.6235\n",
      "Epoch 11/15\n",
      "5339035/5339035 [==============================] - 19s - loss: 0.1841 - acc: 0.7440 - val_loss: 0.1820 - val_acc: 0.7658\n",
      "Epoch 12/15\n",
      "5339035/5339035 [==============================] - 19s - loss: 0.1839 - acc: 0.7452 - val_loss: 0.1820 - val_acc: 0.7389\n",
      "Epoch 13/15\n",
      "5339035/5339035 [==============================] - 19s - loss: 0.1838 - acc: 0.7462 - val_loss: 0.1820 - val_acc: 0.7313\n",
      "Epoch 14/15\n",
      "5339035/5339035 [==============================] - 19s - loss: 0.1836 - acc: 0.7452 - val_loss: 0.1838 - val_acc: 0.6904\n",
      "Epoch 15/15\n",
      "5339035/5339035 [==============================] - 19s - loss: 0.1835 - acc: 0.7472 - val_loss: 0.1820 - val_acc: 0.7801\n",
      "2455000/2542399 [===========================>..] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "model = nn_model()\n",
    "model.fit(train_x, train_labels, epochs=15, verbose=1, validation_split=0.1, batch_size=5000, shuffle=True)\n",
    "pred_prob = model.predict_proba(test_x, batch_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.2093073525457366, 0.25989632542949426, 0.2074950323576962)\n"
     ]
    }
   ],
   "source": [
    "print(fscore_nn(test, pred_prob, model, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train\n",
    "del test\n",
    "del train_x\n",
    "del test_x\n",
    "del train_labels\n",
    "del test_labels\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### build candidates list for test ###\n",
    "\n",
    "#前面搞好了 \n",
    "df_test = pd.read_pickle('df_test')\n",
    "#df_test, _ = features(test_orders)\n",
    "\n",
    "#clf.fit(df_train[f_to_use],labels)\n",
    "d_train = xgb.DMatrix(df_train[f_to_use],label=labels)\n",
    "d_d=xgb.DMatrix(df_test[f_to_use])   \n",
    "\n",
    "bst = xgb.train(params_xgb, d_train, ROUNDS)\n",
    "df_test['pred'] = bst.predict(d_d)\n",
    "\n",
    "sub=get_pred_results(df_test,0.22)\n",
    "#sub.to_csv('sub.csv', index=False)\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "sub.to_csv('xgb_results_{}.{}.{}.csv'.format(\n",
    "    str(now.date()),\n",
    "    str(now.hour),\n",
    "    str(now.minute)\n",
    "), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "8474661/8474661 [==============================] - 10s - loss: 0.1939 - acc: 0.7291    \n",
      "Epoch 2/20\n",
      "8474661/8474661 [==============================] - 10s - loss: 0.1883 - acc: 0.7357    \n",
      "Epoch 3/20\n",
      "8474661/8474661 [==============================] - 10s - loss: 0.1868 - acc: 0.7399    \n",
      "Epoch 4/20\n",
      "8474661/8474661 [==============================] - 10s - loss: 0.1857 - acc: 0.7422    \n",
      "Epoch 5/20\n",
      "8474661/8474661 [==============================] - 11s - loss: 0.1850 - acc: 0.7441    \n",
      "Epoch 6/20\n",
      "8474661/8474661 [==============================] - 11s - loss: 0.1846 - acc: 0.7445    \n",
      "Epoch 7/20\n",
      "8474661/8474661 [==============================] - 11s - loss: 0.1842 - acc: 0.7457    \n",
      "Epoch 8/20\n",
      "8474661/8474661 [==============================] - 10s - loss: 0.1838 - acc: 0.7463    \n",
      "Epoch 9/20\n",
      "8474661/8474661 [==============================] - 10s - loss: 0.1836 - acc: 0.7471    \n",
      "Epoch 10/20\n",
      "8474661/8474661 [==============================] - 10s - loss: 0.1834 - acc: 0.7482    \n",
      "Epoch 11/20\n",
      "8474661/8474661 [==============================] - 10s - loss: 0.1832 - acc: 0.7486    \n",
      "Epoch 12/20\n",
      "8474661/8474661 [==============================] - 10s - loss: 0.1830 - acc: 0.7498    \n",
      "Epoch 13/20\n",
      "8474661/8474661 [==============================] - 10s - loss: 0.1828 - acc: 0.7502    \n",
      "Epoch 14/20\n",
      "8474661/8474661 [==============================] - 10s - loss: 0.1828 - acc: 0.7509    \n",
      "Epoch 15/20\n",
      "8474661/8474661 [==============================] - 10s - loss: 0.1826 - acc: 0.7504    \n",
      "Epoch 16/20\n",
      "8474661/8474661 [==============================] - 10s - loss: 0.1825 - acc: 0.7516    \n",
      "Epoch 17/20\n",
      "8474661/8474661 [==============================] - 10s - loss: 0.1824 - acc: 0.7518    \n",
      "Epoch 18/20\n",
      "8474661/8474661 [==============================] - 10s - loss: 0.1824 - acc: 0.7524    \n",
      "Epoch 19/20\n",
      "8474661/8474661 [==============================] - 10s - loss: 0.1823 - acc: 0.7523    \n",
      "Epoch 20/20\n",
      "8474661/8474661 [==============================] - 10s - loss: 0.1822 - acc: 0.7521    \n",
      "4720000/4833292 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_pickle('df_test')\n",
    "#df_test, _ = features(test_orders)\n",
    "\n",
    "train_x = scalerX.transform(df_train[f_to_use])\n",
    "test_x = scalerX.transform(df_test[f_to_use])\n",
    "\n",
    "model = nn_model()\n",
    "model.fit(train_x, labels, epochs=20, verbose=1, batch_size=5000, shuffle=True)\n",
    "\n",
    "df_test['pred'] = model.predict_proba(test_x,  batch_size=5000)\n",
    "sub=get_pred_results(df_test,0.5)\n",
    "\n",
    "#sub.to_csv('sub.csv', index=False)\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "sub.to_csv('nn_results_{}.{}.{}.csv'.format(\n",
    "    str(now.date()),\n",
    "    str(now.hour),\n",
    "    str(now.minute)\n",
    "), index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
