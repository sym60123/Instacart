{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['order_id', 'product_id', 'user_total_orders', 'user_total_items',\n",
      "       'total_distinct_items', 'user_average_days_between_orders',\n",
      "       'user_average_basket', 'percent_all_reordered', 'user_reorder_ratio',\n",
      "       'dow', 'order_hour_of_day', 'order_hour_of_week',\n",
      "       'days_since_prior_order', 'days_since_ratio', 'aisle_id',\n",
      "       'department_id', 'product_orders', 'product_reorders',\n",
      "       'product_reorder_rate', 'department_orders', 'department_reorders',\n",
      "       'department_reorder_rate', 'aisle_orders', 'aisle_reorders',\n",
      "       'aisle_reorder_rate', 'is_Organic', 'prod_average_pos_in_cart',\n",
      "       'UP_orders', 'UP_reorders', 'UP_orders_ratio', 'UP_average_pos_in_cart',\n",
      "       'UP_reorder_rate', 'UP_orders_since_last', 'UP_last_order_number',\n",
      "       'UP_first_order_number', 'UP_order_rate_since_first_order',\n",
      "       'UP_delta_hour_vs_last', 'UP_distance_radio_since_last_order',\n",
      "       'UP_in_last_order', 'UP_same_dow_as_last_order', 'labels'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#products = pd.read_pickle('products')\n",
    "#priors = pd.read_pickle('priors')\n",
    "#users = pd.read_pickle('users')\n",
    "#userXproduct = pd.read_pickle('userXproduct')\n",
    "df_temp_new = pd.read_pickle('df_temp_new')\n",
    "df_train_new = pd.read_pickle('df_train_new')\n",
    "print(df_train_new.columns)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206209\n",
      "[[  13.79774761  155.47648621]\n",
      " [   7.60240602   41.61329651]]\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import *\n",
    "import time  \n",
    "import scipy\n",
    "import matplotlib.pyplot as plt  \n",
    "import pandas as pd\n",
    "  \n",
    "  \n",
    "# calculate Euclidean distance  \n",
    "def euclDistance(vector1, vector2):  \n",
    "    return sqrt(sum(power(vector2 - vector1, 2)))  \n",
    "  \n",
    "# init centroids with random samples  \n",
    "def initCentroids(dataSet, k):  \n",
    "    numSamples, dim = dataSet.shape  \n",
    "    centroids = zeros((k, dim))  \n",
    "    for i in range(k):  \n",
    "        index = int(random.uniform(0, numSamples))  \n",
    "        centroids[i, :] = dataSet[index, :]  \n",
    "    return centroids  \n",
    "  \n",
    "# k-means cluster  \n",
    "def kmeans(dataSet, k):  \n",
    "    numSamples = dataSet.shape[0]  \n",
    "    print (numSamples)\n",
    "    # first column stores which cluster this sample belongs to,  \n",
    "    # second column stores the error between this sample and its centroid  \n",
    "    clusterAssment = mat(zeros((numSamples, 2)))  \n",
    "    clusterChanged = True  \n",
    "  \n",
    "    ## step 1: init centroids  \n",
    "    centroids = initCentroids(dataSet, k)  \n",
    "  \n",
    "    while clusterChanged:  \n",
    "        clusterChanged = False  \n",
    "        ## for each sample  \n",
    "        for i in range(numSamples):  \n",
    "            minDist  = 100000.0  \n",
    "            minIndex = 0  \n",
    "            ## for each centroid  \n",
    "            ## step 2: find the centroid who is closest  \n",
    "            for j in range(k):  \n",
    "                distance = euclDistance(centroids[j, :], dataSet[i, :])  \n",
    "                if distance < minDist:  \n",
    "                    minDist  = distance  \n",
    "                    minIndex = j  \n",
    "              \n",
    "            ## step 3: update its cluster  \n",
    "            if clusterAssment[i, 0] != minIndex:  \n",
    "                clusterChanged = True  \n",
    "                clusterAssment[i, :] = minIndex, minDist**2  \n",
    "  \n",
    "        ## step 4: update centroids  \n",
    "        for j in range(k):  \n",
    "            pointsInCluster = dataSet[nonzero(clusterAssment[:, 0].A == j)[0]]  \n",
    "            centroids[j, :] = mean(pointsInCluster, axis = 0)  \n",
    "   \n",
    "    return centroids, clusterAssment\n",
    "\n",
    "df_train_dic = {}\n",
    "for row in df_train_new.itertuples():\n",
    "    if row.order_id not in df_train_dic.keys():\n",
    "        df_train_dic[row.order_id] = [row.user_average_basket,row.total_distinct_items]\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "df_test_new = pd.read_pickle('df_test_new')\n",
    "df_test_dic = {}\n",
    "for row in df_test_new.itertuples():\n",
    "    if row.order_id not in df_test_dic.keys():\n",
    "        df_test_dic[row.order_id] = [row.user_average_basket,row.total_distinct_items]\n",
    "    else:\n",
    "        continue\n",
    "kmeans_list = []\n",
    "for keys in df_train_dic.keys():\n",
    "    tmp = [keys] + df_train_dic[keys]\n",
    "    kmeans_list.append(tmp)\n",
    "for keys in df_test_dic.keys():\n",
    "    tmp = [keys] + df_test_dic[keys]\n",
    "    kmeans_list.append(tmp)\n",
    "kmeans_list_in = []\n",
    "for i in range(len(kmeans_list)):\n",
    "    kmeans_list_in.append(kmeans_list[i][1:])\n",
    "\n",
    "kmeans_list_in = mat(kmeans_list_in)\n",
    "k = 2\n",
    "centroids, clusterAssment = kmeans(kmeans_list_in, k) \n",
    "clusterAssment = clusterAssment.tolist()\n",
    "kmeans_out = {}\n",
    "for i in range(len(kmeans_list)):\n",
    "    kmeans_out[kmeans_list[i][0]] = int(clusterAssment[i][0])\n",
    "\n",
    "print(centroids)\n",
    "print(kmeans_out[1187899])\n",
    "del df_train_dic\n",
    "del df_test_dic\n",
    "del kmeans_list\n",
    "del kmeans_list_in\n",
    "del clusterAssment\n",
    "del df_test_new\n",
    "gc.collect()\n",
    "#data = pd.DataFrame(clusterAssment)\n",
    "#a = data[0].tolist()\n",
    "#print (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nf_to_use = ['user_total_orders', 'user_average_days_between_orders', \\n       'order_hour_of_day', 'days_since_prior_order',\\n       'aisle_id', 'department_id', 'product_orders', 'product_reorders',\\n       'UP_orders', 'UP_orders_ratio', 'UP_reorder_rate', 'UP_orders_since_last',\\n       'UP_delta_hour_vs_last'] # 'dow', 'UP_same_dow_as_last_order'\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.array(df_train_new['labels'],dtype=pd.Series)\n",
    "df_train_new.drop(['labels'],axis=1,inplace=True)\n",
    "\n",
    "f_to_use = ['user_total_orders', 'user_total_items',\n",
    "       'total_distinct_items', 'user_average_days_between_orders',\n",
    "       'user_average_basket', 'percent_all_reordered', 'user_reorder_ratio',\n",
    "       'dow', 'order_hour_of_day', 'order_hour_of_week',\n",
    "       'days_since_prior_order', 'days_since_ratio', 'aisle_id',\n",
    "       'department_id', 'product_orders', 'product_reorders',\n",
    "       'product_reorder_rate', 'department_orders', 'department_reorders',\n",
    "       'department_reorder_rate', 'aisle_orders', 'aisle_reorders',\n",
    "       'aisle_reorder_rate', 'is_Organic', 'prod_average_pos_in_cart',\n",
    "       'UP_orders', 'UP_reorders', 'UP_orders_ratio', 'UP_average_pos_in_cart',\n",
    "       'UP_reorder_rate', 'UP_orders_since_last', 'UP_last_order_number',\n",
    "       'UP_first_order_number', 'UP_order_rate_since_first_order',\n",
    "       'UP_delta_hour_vs_last', 'UP_distance_radio_since_last_order',\n",
    "       'UP_in_last_order', 'UP_same_dow_as_last_order'] # 'dow', 'UP_same_dow_as_last_order'\n",
    "'''\n",
    "f_to_use = ['user_total_orders', 'user_average_days_between_orders', \n",
    "       'order_hour_of_day', 'days_since_prior_order',\n",
    "       'aisle_id', 'department_id', 'product_orders', 'product_reorders',\n",
    "       'UP_orders', 'UP_orders_ratio', 'UP_reorder_rate', 'UP_orders_since_last',\n",
    "       'UP_delta_hour_vs_last'] # 'dow', 'UP_same_dow_as_last_order'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_fun(labels, preds):\n",
    "    labels = labels.split(' ')\n",
    "    preds = preds.split(' ')\n",
    "    rr = (np.intersect1d(labels, preds))\n",
    "    precision = np.float(len(rr)) / len(preds)\n",
    "    recall = np.float(len(rr)) / len(labels)\n",
    "    try:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "        return (precision, recall, 0.0)\n",
    "    return (precision, recall, f1)\n",
    "\n",
    "def get_pred_results(df_test,thrshold):\n",
    "    d = dict()\n",
    "    #TRESHOLD = thrshold\n",
    "    for row in df_test.itertuples():\n",
    "        TRESHOLD = thrshold[kmeans_out[row.order_id]]                \n",
    "        if row.pred > TRESHOLD:\n",
    "            try:\n",
    "                d[row.order_id] += ' ' + str(row.product_id)\n",
    "            except:\n",
    "                d[row.order_id] = str(row.product_id)\n",
    "    for order in df_test.order_id:\n",
    "        if order not in d:\n",
    "            d[order] = 'None'\n",
    "\n",
    "    sub = pd.DataFrame.from_dict(d, orient='index')\n",
    "    sub.reset_index(inplace=True)\n",
    "    sub.columns = ['order_id', 'products']\n",
    "    return sub\n",
    "\n",
    "\n",
    "def fscore(df,bst,alpha):\n",
    "    df['pred'] = bst.predict(df[f_to_use])\n",
    "    train_pred=get_pred_results(df,thrshold=alpha)\n",
    "    #合表\n",
    "    train_pred1=pd.merge(train_pred,df_temp_new,on=['order_id'])\n",
    "    #求F1结果表\n",
    "    res = list()\n",
    "    for entry in train_pred1.itertuples():\n",
    "        res.append(eval_fun(entry[2], entry[3]))\n",
    "    res = pd.DataFrame(np.array(res), columns=['precision', 'recall', 'f1'])\n",
    "    return res['f1'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 1: train:0.39481814075187593, test:0.3949530602238411\n",
      "* 2: train:0.3966529824829051, test:0.39180402850745993\n",
      "* 3: train:0.39674947999662047, test:0.39181243661259685\n",
      "[0.15, 0.17]\n",
      "ALL:train:0.39607353441046717 test:0.392856508447966\n",
      "cost time:934.751403755974\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#先使用最简单的k-Fold\n",
    "from sklearn.model_selection import KFold\n",
    "from lightgbm import LGBMClassifier\n",
    "thrshold_list = [[0.15,0.17]]\n",
    "for i in range(len(thrshold_list)):\n",
    "    thrshold_now = thrshold_list[i]\n",
    "    params = {\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': {'binary_logloss'},\n",
    "        'num_leaves': 96,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.95,\n",
    "        'bagging_freq': 5\n",
    "    }\n",
    "    ROUNDS = 98\n",
    "    kf=KFold(n_splits=3)    # 定义分成几个组\n",
    "    list_f1=[]\n",
    "    list_f2=[]\n",
    "    num=1\n",
    "    #clf=LGBMClassifier(objective='binary', boosting_type='gbdt')\n",
    "    #决定采用手动cv  因为需要了利用合表才能得到F1 传统方法不可以 数组合表 太可怕···\n",
    "    #把数组变成切边的形式  即可\n",
    "    import timeit\n",
    "    start=timeit.default_timer()\n",
    "\n",
    "    for train_index,test_index in kf.split(df_train_new, labels):\n",
    "\n",
    "        #train_max=train_index.max()\n",
    "        #train_min=train_index.min()\n",
    "        test_max=test_index.max()+1\n",
    "        test_min=test_index.min()\n",
    "        X_test=df_train_new[test_min:test_max]\n",
    "        X_train=df_train_new.drop(test_index)\n",
    "        #X_train,X_test=data_train[train_index],data_train[test_index]\n",
    "        y_train,y_test=labels[train_index],labels[test_index]   \n",
    "        d_train = lgb.Dataset(X_train[f_to_use],\n",
    "                          label=y_train,\n",
    "                          categorical_feature=['aisle_id', 'department_id'])  # , 'order_hour_of_day', 'dow'\n",
    "        bst = lgb.train(params, d_train, ROUNDS)\n",
    "        a=fscore(X_train,bst,thrshold_now)\n",
    "        b=fscore(X_test,bst,thrshold_now)\n",
    "        list_f1.append(a)\n",
    "        list_f2.append(b)\n",
    "        print('* {}: train:{}, test:{}'.format(num,a,b))\n",
    "        num+=1\n",
    "    print (thrshold_now)\n",
    "    print('ALL:train:{} test:{}'.format(np.mean(list_f1),np.mean(list_f2)))\n",
    "    end = timeit.default_timer()\n",
    "    print('cost time:'+str(end-start))\n",
    "\n",
    "    del d_train\n",
    "    del X_test\n",
    "    del X_train\n",
    "    del y_train\n",
    "    del y_test\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_new = pd.read_pickle('df_test_new')\n",
    "\n",
    "#clf.fit(df_train[f_to_use],labels)\n",
    "d_train = lgb.Dataset(df_train_new[f_to_use],label=labels,categorical_feature=['aisle_id', 'department_id'])\n",
    "#d_d=lgb.Dataset(df_test[f_to_use],categorical_feature=['aisle_id', 'department_id'])   \n",
    "\n",
    "bst = lgb.train(params, d_train, ROUNDS)\n",
    "df_test_new['pred'] = bst.predict(df_test_new[f_to_use])\n",
    "\n",
    "sub=get_pred_results(df_test_new,[0.15,0.17])\n",
    "#sub.to_csv('sub.csv', index=False)\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "sub.to_csv('lgb_results_{}.{}.{}.csv'.format(\n",
    "    str(now.date()),\n",
    "    str(now.hour),\n",
    "    str(now.minute)\n",
    "), index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shisaijiang/anaconda/envs/python36/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "del df_train_use\n",
      "del train_list\n",
      "Creating validation dataset of 0.01 of training for adaptive regularization\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from pyfm import pylibfm\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "df_train_use = df_train[f_to_use]\n",
    "train_key = df_train_use.columns.tolist()\n",
    "\n",
    "train_list = []\n",
    "for row in df_train_use.itertuples():\n",
    "    tmp = {}\n",
    "    for i in range(len(train_key)):\n",
    "        tmp[train_key[i]] = row[i+1]\n",
    "    train_list.append(tmp)\n",
    "print('del df_train_use')\n",
    "del df_train_use\n",
    "del df_train\n",
    "gc.collect()\n",
    "\n",
    "v = DictVectorizer()\n",
    "X = v.fit_transform(train_list)\n",
    "print('del train_list')\n",
    "del train_list\n",
    "gc.collect()\n",
    "\n",
    "y = labels\n",
    "fm = pylibfm.FM(num_factors=50, num_iter=10, verbose=True, task=\"classification\", initial_learning_rate=0.0001, learning_rate_schedule=\"optimal\")\n",
    "fm.fit(X,y)\n",
    "\n",
    "df_test = pd.read_pickle('df_test')\n",
    "df_test_use = df_test[f_to_use]\n",
    "test_list = []\n",
    "for row in df_test_use.itertuples():\n",
    "    tmp = {}\n",
    "    for i in range(len(train_key)):\n",
    "        tmp[train_key[i]] = row[i+1]\n",
    "    test_list.append(tmp)\n",
    "del df_test_use\n",
    "gc.collect()\n",
    "X_test = v.transform(test_list)\n",
    "del test_list\n",
    "gc.collect()\n",
    "pp = fm.predict(X_test)\n",
    "df_test['pred'] = pp\n",
    "sub=get_pred_results(df_test,0.22)\n",
    "#sub.to_csv('sub.csv', index=False)\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "sub.to_csv('fm_results_{}.{}.{}.csv'.format(\n",
    "    str(now.date()),\n",
    "    str(now.hour),\n",
    "    str(now.minute)\n",
    "), index = False)\n",
    "\n",
    "'''\n",
    "train = [\n",
    "    {\"user\": \"1\", \"item\": \"5\", \"age\": 19},\n",
    "    {\"user\": \"2\", \"item\": \"43\", \"age\": 33},\n",
    "    {\"user\": \"3\", \"item\": \"20\", \"age\": 55},\n",
    "    {\"user\": \"4\", \"item\": \"10\", \"age\": 20},\n",
    "]\n",
    "train1 = [\n",
    "    {\"user\": 1, \"item\": 5, \"age\": 19},\n",
    "    {\"user\": 2, \"item\": 43, \"age\": 33},\n",
    "    {\"user\": 3, \"item\": 20, \"age\": 55},\n",
    "    {\"user\": 4, \"item\": 10, \"age\": 20},\n",
    "]\n",
    "v = DictVectorizer()\n",
    "X = v.fit_transform(train)\n",
    "X1 = v.fit_transform(train1)\n",
    "print(X)\n",
    "print(X.toarray())\n",
    "print(X1)\n",
    "print(X1.toarray())\n",
    "y = np.repeat(1.0,X.shape[0])\n",
    "print(y)\n",
    "fm = pylibfm.FM(num_factors=50, num_iter=100, verbose=True, task=\"classification\", initial_learning_rate=0.0001, learning_rate_schedule=\"optimal\")\n",
    "fm.fit(X,y)\n",
    "print(v.transform({\"user\": 1, \"item\": 10, \"age\": 24}))\n",
    "a = fm.predict(v.transform({\"user\": \"1\", \"item\": \"10\", \"age\": 24}))\n",
    "print(a)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
